# configs/african_evaluation.yaml

# African-language-focused evaluation.
# Run with:
#   python scripts/evaluate.py whisper_large_v3 --eval-config stt_benchmark/config/eval_configs/african_eval.yaml

experiment_name: african_eval

# ── ASR ──────────────────────────────────────────────────────────────────────
# Languages to evaluate for automatic speech recognition.
asr:
  languages:
    - af_za    # Afrikaans
    - am_et    # Amharic
    - ha_ng    # Hausa
    - ig_ng    # Igbo
    - lg_ug    # Luganda
    - ln_cd    # Lingala
    - luo_ke   # Luo
    - ny_mw    # Chichewa
    - om_et    # Oromo
    - sn_zw    # Shona
    - so_so    # Somali
    - sw_ke    # Swahili
    - wo_sn    # Wolof
    - xh_za    # Xhosa
    - yo_ng    # Yoruba
    - zu_za    # Zulu

# ── AST ──────────────────────────────────────────────────────────────────────
# Each source language is paired with each anchor.
#
# direction controls how pairs expand:
#   "forward"  — source → anchor  (source audio, anchor text)
#   "reverse"  — anchor → source  (anchor audio, source text)
#   "both"     — both directions
#
# FLEURS parallel data is structured as {source}-{target}.csv where source
# has the audio. So use "forward" to match data like sw_ke-en_us.csv,
# or "both" if your dataset also has en_us-sw_ke.csv.
ast:
  # Sources default to asr.languages when omitted
  # sources:
  #   - sw_ke
  #   - yo_ng

  anchors:
    - en_us    # English
    - fr_fr    # French

  direction: both   # source → anchor AND anchor → source

  # Optional: additional explicit pairs
  # extra_pairs:
  #   - source: ar_eg
  #     target: sw_ke